{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a74b054c-3f5d-4001-b4f3-43850bbbd37f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f54365c-c9fe-4dd3-b322-e892fc467ea6",
   "metadata": {},
   "source": [
    "Se realiza la conexión con la instancia de MySQL creada, con el fin de poder realizar la carga de los datos iniciales y asi poder entrenar el respectivo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbd60e37-1c6f-459a-bb9c-8f8374075fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>culmen_length_mm</th>\n",
       "      <th>culmen_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>MALE\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>FEMALE\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>FEMALE\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>FEMALE\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>340</td>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>341</td>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>46.8</td>\n",
       "      <td>14.3</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4850.0</td>\n",
       "      <td>FEMALE\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>342</td>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>50.4</td>\n",
       "      <td>15.7</td>\n",
       "      <td>222.0</td>\n",
       "      <td>5750.0</td>\n",
       "      <td>MALE\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>343</td>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>45.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>212.0</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>FEMALE\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>344</td>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>49.9</td>\n",
       "      <td>16.1</td>\n",
       "      <td>213.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>MALE\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>344 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id species     island  culmen_length_mm  culmen_depth_mm  \\\n",
       "0      1  Adelie  Torgersen              39.1             18.7   \n",
       "1      2  Adelie  Torgersen              39.5             17.4   \n",
       "2      3  Adelie  Torgersen              40.3             18.0   \n",
       "3      4  Adelie  Torgersen               NaN              NaN   \n",
       "4      5  Adelie  Torgersen              36.7             19.3   \n",
       "..   ...     ...        ...               ...              ...   \n",
       "339  340  Gentoo     Biscoe               NaN              NaN   \n",
       "340  341  Gentoo     Biscoe              46.8             14.3   \n",
       "341  342  Gentoo     Biscoe              50.4             15.7   \n",
       "342  343  Gentoo     Biscoe              45.2             14.8   \n",
       "343  344  Gentoo     Biscoe              49.9             16.1   \n",
       "\n",
       "     flipper_length_mm  body_mass_g       sex  \n",
       "0                181.0       3750.0    MALE\\r  \n",
       "1                186.0       3800.0  FEMALE\\r  \n",
       "2                195.0       3250.0  FEMALE\\r  \n",
       "3                  NaN          NaN      NA\\r  \n",
       "4                193.0       3450.0  FEMALE\\r  \n",
       "..                 ...          ...       ...  \n",
       "339                NaN          NaN      NA\\r  \n",
       "340              215.0       4850.0  FEMALE\\r  \n",
       "341              222.0       5750.0    MALE\\r  \n",
       "342              212.0       5200.0  FEMALE\\r  \n",
       "343              213.0       5400.0    MALE\\r  \n",
       "\n",
       "[344 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "# Conexión a la base de datos MySQL\n",
    "conexion = mysql.connector.connect(\n",
    "    host='mysql_model_data',         # Usamos el nombre del servicio en Docker\n",
    "    user='admin',\n",
    "    password='supersecret',\n",
    "    database='mydatabase'\n",
    ")\n",
    "\n",
    "# Consulta SQL\n",
    "query = \"SELECT * FROM penguins\"  # Reemplaza 'tu_tabla' por el nombre de la tabla que necesites\n",
    "\n",
    "# Ejecutamos la consulta y obtenemos los resultados con sus nombres de columna\n",
    "cursor = conexion.cursor()\n",
    "cursor.execute(query)\n",
    "datos = cursor.fetchall()\n",
    "columnas = cursor.column_names\n",
    "\n",
    "# Creamos un DataFrame de pandas con la información obtenida\n",
    "df_view = pd.DataFrame(datos, columns=columnas)\n",
    "\n",
    "# Cerramos cursor y conexión\n",
    "cursor.close()\n",
    "conexion.close()\n",
    "\n",
    "# Mostramos el DataFrame\n",
    "df_view\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fcc9b6-14cd-4839-8257-6ca5f4b6680b",
   "metadata": {},
   "source": [
    "Limpieza de datos innecesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "102311b2-444a-423a-86c3-c0205b8bc108",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>culmen_length_mm</th>\n",
       "      <th>culmen_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>MALE\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>FEMALE\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>FEMALE\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>FEMALE\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.3</td>\n",
       "      <td>20.6</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>MALE\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>339</td>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>47.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>214.0</td>\n",
       "      <td>4925.0</td>\n",
       "      <td>FEMALE\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>341</td>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>46.8</td>\n",
       "      <td>14.3</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4850.0</td>\n",
       "      <td>FEMALE\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>342</td>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>50.4</td>\n",
       "      <td>15.7</td>\n",
       "      <td>222.0</td>\n",
       "      <td>5750.0</td>\n",
       "      <td>MALE\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>343</td>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>45.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>212.0</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>FEMALE\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>344</td>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>49.9</td>\n",
       "      <td>16.1</td>\n",
       "      <td>213.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>MALE\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>342 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id species     island  culmen_length_mm  culmen_depth_mm  \\\n",
       "0      1  Adelie  Torgersen              39.1             18.7   \n",
       "1      2  Adelie  Torgersen              39.5             17.4   \n",
       "2      3  Adelie  Torgersen              40.3             18.0   \n",
       "4      5  Adelie  Torgersen              36.7             19.3   \n",
       "5      6  Adelie  Torgersen              39.3             20.6   \n",
       "..   ...     ...        ...               ...              ...   \n",
       "338  339  Gentoo     Biscoe              47.2             13.7   \n",
       "340  341  Gentoo     Biscoe              46.8             14.3   \n",
       "341  342  Gentoo     Biscoe              50.4             15.7   \n",
       "342  343  Gentoo     Biscoe              45.2             14.8   \n",
       "343  344  Gentoo     Biscoe              49.9             16.1   \n",
       "\n",
       "     flipper_length_mm  body_mass_g       sex  \n",
       "0                181.0       3750.0    MALE\\r  \n",
       "1                186.0       3800.0  FEMALE\\r  \n",
       "2                195.0       3250.0  FEMALE\\r  \n",
       "4                193.0       3450.0  FEMALE\\r  \n",
       "5                190.0       3650.0    MALE\\r  \n",
       "..                 ...          ...       ...  \n",
       "338              214.0       4925.0  FEMALE\\r  \n",
       "340              215.0       4850.0  FEMALE\\r  \n",
       "341              222.0       5750.0    MALE\\r  \n",
       "342              212.0       5200.0  FEMALE\\r  \n",
       "343              213.0       5400.0    MALE\\r  \n",
       "\n",
       "[342 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_view.dropna(inplace=True)\n",
    "df_view = df_view[df_view['sex']!='.']\n",
    "df_view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d224335-c842-4c50-bf71-780a2add8d7e",
   "metadata": {},
   "source": [
    "Selección de muestra de entrenamiento y validación, para el flujo de entrenamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe93b6b0-7825-41bb-a2f0-cb41051742bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df_view.drop(columns=[\"species\",\"id\"])\n",
    "y = df_view[\"species\"]\n",
    "text_cols = [\"island\", \"sex\"]\n",
    "num_cols = [\n",
    "    \"culmen_length_mm\", \"culmen_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"\n",
    "]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed90649-619f-4909-a400-3f14fb008e9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Se define el pipeline que permitira la transformación de las variables categoricas con OneHotEncoder y es estandariza la escala de las variables númericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7b80dc9-44ca-427e-8946-2acf7125b62b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "    (\"text\", OneHotEncoder(handle_unknown=\"ignore\"), text_cols),\n",
    "    (\"num\", StandardScaler(), num_cols)\n",
    "])\n",
    "\n",
    "modelRF = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabaf488-6fbc-42fb-9ec5-1414a07e3588",
   "metadata": {},
   "source": [
    "Ahora se procede a la definición de la Grilla la cual contendra todos los parametros sujetos al ajuste de los modelos, garantizando que se tengan 21 combinaciones de hiperparametros diferentes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3c025280-bf60-45c2-a165-e3c51549c773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definición de la grilla de parámetros para el RandomForestClassifier\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [10, 50, 100],\n",
    "    \"classifier__max_depth\": [ 5, 8, 10],\n",
    "    \"classifier__min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=modelRF,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,               # Número de folds para validación cruzada\n",
    "    scoring=\"accuracy\", # Métrica de evaluación\n",
    "    n_jobs=-1,          # Utiliza todos los cores disponibles\n",
    "    verbose=2           # Muestra información del progreso\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "be22c149-b9ad-407a-8ece-9c574f192584",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/17 19:50:01 INFO mlflow.tracking.fluent: Experiment with name 'mlflow_penguins_vf' does not exist. Creating a new experiment.\n",
      "2025/03/17 19:50:02 WARNING mlflow.utils: Truncated the value of the key `estimator`. Truncated value: `Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('text',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  ['island', 'sex']),\n",
      "                                                 ('num', StandardScaler(),\n",
      "                                                  ['culmen_length_mm',\n",
      "                                                   'culmen_depth_mm',\n",
      "                              ...`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'modelo_penguins'.\n",
      "2025/03/17 19:50:13 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: modelo_penguins, version 1\n",
      "Created version '1' of model 'modelo_penguins'.\n",
      "2025/03/17 19:50:16 INFO mlflow.sklearn.utils: Logging the 27 best runs, no runs will be omitted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=2, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.4s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=10, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=10, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=10, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=10, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=2, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=10, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=10, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=2, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=10, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=10, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=5, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=5, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=10, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=2, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=2, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=5, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=5, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=5, classifier__n_estimators=10; total time=   0.1s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=10, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=2, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=2, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=10, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=10, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=2, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=5, classifier__n_estimators=10; total time=   0.1s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=5, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=10, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=10, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=10, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=5, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=2, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=2, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=5, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=5, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=5, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=10, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=10, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=10, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=10, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=10, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=8, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=10; total time=   0.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.2s\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import os\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL'] = \"http://10.43.101.197:9000\"\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'admin'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'supersecret'\n",
    "\n",
    "# connect to mlflow\n",
    "mlflow.set_tracking_uri(\"http://10.43.101.197:5000\")\n",
    "mlflow.set_experiment(\"mlflow_penguins_vf\")\n",
    "\n",
    "mlflow.sklearn.autolog(log_model_signatures=True, log_input_examples=True, registered_model_name=\"modelo_penguins\", max_tuning_runs=27)\n",
    "\n",
    "with mlflow.start_run(run_name=\"autolog_pipe_model_reg\") as run:\n",
    "    grid_search.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
