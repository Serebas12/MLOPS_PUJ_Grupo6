#           Desarrollo Proyecto 3   

Este repositorio contiene los archivos, configuraciones y recursos necesarios para desplegar un entorno completo de MLOps utilizando Kubernetes, integrando los servicios de Airflow, MLflow, Prometheus, Grafana, FastAPI y Streamlit. Este ecosistema robusto permite implementar un flujo de trabajo integral que abarca la ingesta, procesamiento, modelado, registro, despliegue y monitoreo de modelos de machine learning, todo gestionado y automatizado mediante DAGs orquestados con Airflow.

El proyecto forma parte del curso de Operaciones de Machine Learning de la Pontificia Universidad Javeriana y tiene como objetivo no solo encontrar el mejor modelo posible para el conjunto de datos propuesto (encuentros hospitalarios relacionados con diabetes en EE.UU. entre 1999 y 2008), sino tambi√©n dise√±ar e implementar una arquitectura moderna que refleje las mejores pr√°cticas de MLOps, poniendo √©nfasis en la trazabilidad, el versionamiento, la automatizaci√≥n y la escalabilidad.

El flujo del proceso incluye:

*   La recolecci√≥n e ingesti√≥n de datos mediante pipelines controlados por Airflow.

*   El almacenamiento de datos en bases especializadas.

*   El preprocesamiento y entrenamiento peri√≥dico de modelos.

*   El registro autom√°tico de los resultados en MLflow.

*   La selecci√≥n y publicaci√≥n autom√°tica del modelo √≥ptimo al entorno de producci√≥n.

*   El consumo del modelo en l√≠nea a trav√©s de una API desarrollada en FastAPI.

*   La exposici√≥n de resultados al usuario final mediante una interfaz interactiva construida con Streamlit.

*   Y la integraci√≥n de herramientas de monitoreo como Prometheus y Grafana, junto a pruebas de carga con Locust, para garantizar la resiliencia y desempe√±o del sistema en condiciones reales de operaci√≥n.

La entrega final incluye el c√≥digo fuente en un repositorio p√∫blico, el despliegue funcional sobre Kubernetes, y una sustentaci√≥n en video explicando la arquitectura, los procesos implementados y las m√©tricas obtenidas, demostrando as√≠ la capacidad del sistema para operar como un flujo MLOps completamente automatizado y trazable.

---

##          Estructura del Directorio

A continuaci√≥n presentamos la estructura del directorio del proyecto, donde organizamos los diferentes objetos y elementos necesarios para el despliegue de todos los servicios, usando inicialmente **Docker** para crear la arquitectura base y realizar nuestras primeras pruebas, y luego, mediante el uso de Kompose traducir los scripts en los respectivos manifiestos para desplegar **Kubernetes**

Tambi√©n se recalca la necesidad de haber realizado el escrito de 3 `docker-compose`, con el objetivo de poder desplegar `aiflow`, `jupyterlab` y el resto de los servicios, esto con el prop√≥sito de poderlos traducir y llevar a Kubernetes. 

```plaintext
üìÅ PROYECTO3
‚îú‚îÄ‚îÄ üìÅ airflow 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ requirements.txt
‚îú‚îÄ‚îÄ üìÅ app 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ main.py
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ requirements.txt
‚îú‚îÄ‚îÄ üìÅ dags 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ cleandata_pipeline.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ insert_rawdata_init.py
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ modeling.py
‚îú‚îÄ‚îÄ üìÅ data 
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ Diabetes.csv
‚îú‚îÄ‚îÄ üìÅ images                                   #   Imagenes
‚îú‚îÄ‚îÄ üìÅ initdb 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 01_schemas.sql
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ Dockerfile
‚îú‚îÄ‚îÄ üìÅ jupyter 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ ProfileReport_2025-04-30_diabetes_class.html
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ penguins_size.csv
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ pruebas.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ pruebas2.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ requirements.txt
‚îú‚îÄ‚îÄ üìÅ kompose 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ fast-api-deployment.yaml 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ fast-api-service.yaml 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ grafana-deployment.yaml 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ grafana-service.yaml 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ locust-deployment.yaml 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ locust-service.yaml 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ minio-deployment.yaml 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ minio-service.yaml 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ minio-data-persistentvolumeclaim.yaml 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ mlflow-deployment.yaml 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ mlflow-service.yaml 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ mlops-postgres-deployment.yaml 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ mlops-postgres-service.yaml 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ postgres-mlflow-persistentvolumeclaim.yaml
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ prometheus-configmap.yaml 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ prometheus-deployment.yaml 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ prometheus-service.yaml 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ streamlit-deployment.yaml 
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ streamlit-service.yaml 
‚îú‚îÄ‚îÄ üìÅ locust 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ locustfile.py
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ requirements-locust.txt 
‚îú‚îÄ‚îÄ üìÅ mlflow 
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ Dockerfile
‚îú‚îÄ‚îÄ üìÅ streamlit 
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ streamlit_app.py
‚îú‚îÄ‚îÄ üìÑ .env 
‚îú‚îÄ‚îÄ üìÑ README.md 
‚îú‚îÄ‚îÄ üìÑ docker-compose-airflow.yaml 
‚îú‚îÄ‚îÄ üìÑ docker-compose-jupyter.yaml 
‚îú‚îÄ‚îÄ üìÑ docker-compose-kubernete.yaml 
‚îú‚îÄ‚îÄ üìÑ docker-compose-resto-back.yaml 
‚îî‚îÄ‚îÄ üìÑ prometheus.yml 
```

Esta organizaci√≥n modular permite una gesti√≥n eficiente de cada componente del flujo de datos y facilita la escalabilidad del entorno.

##          Primera Fase (Despliegue por Docker)

Como primer pase del proceso de despliegue, realizamos todo el levantamiento inicial de los servicios utilizando **Docker**, ya que esto nos permite validar en un entorno controlado que las im√°genes construidas funcionan correctamente antes de trasladarlas a un entorno m√°s complejo como **Kubernetes**. De esta forma, aseguramos que cada contenedor tiene el comportamiento esperado y que las dependencias entre servicios est√°n resueltas.

###         Paso 1: Crear Red Compartida    

Primero creamos una red interna en Docker que permitir√° que todos los servicios definidos en los distintos docker-compose se comuniquen entre s√≠:

```bash
sudo docker network create airflow_backend
```

Esta red se usar√° para interconectar los servicios como Airflow, PostgreSQL, MLflow, Streamlit y otros, garantizando que puedan referenciarse por nombre de contenedor.

###         Paso 2: Desplegar Airflow   

Airflow es el orquestador central del pipeline. Su despliegue incluye tambi√©n la base de datos PostgreSQL para almacenar metadatos de ejecuci√≥n. Iniciamos el servicio con los siguientes comandos:

```bash
sudo docker compose -f docker-compose-airflow.yaml up airflow-init
sudo docker compose -f docker-compose-airflow.yaml up --build -d
```

El primer comando (airflow-init) prepara la base de datos y las configuraciones iniciales. El segundo comando levanta los contenedores en modo desacoplado (-d) y reconstruye las im√°genes si es necesario (--build).

###         Paso 3: Desplegar servicios complementarios

Una vez Airflow est√° en ejecuci√≥n, procedemos a levantar todos los servicios adicionales definidos para el entorno MLOps (MLflow, Prometheus, Grafana, FastAPI, Streamlit, etc.) con:

```bash
sudo docker compose -f docker-compose-kubernete.yaml up --build -d
```

Esto asegura que todos los servicios est√©n disponibles y accesibles dentro de la red Docker.

###         Paso 4: Desplegar JupyterLab (Opcional)

Si queremos habilitar un entorno interactivo para exploraci√≥n, desarrollo y prueba de c√≥digo, podemos desplegar JupyterLab:

```bash
sudo docker compose -f docker-compose-jupyter.yaml up --build -d
```

Este contenedor nos permite conectarnos a los datos y servicios ya levantados para realizar pruebas manuales o an√°lisis exploratorio.

###         Bajar Servicios 

En caso de necesitar apagar o reconstruir los servicios, usamos los siguientes comandos para derribar los entornos espec√≠ficos, eliminando tambi√©n las im√°genes y vol√∫menes asociados:

```bash
sudo docker compose -f docker-compose-airflow.yaml down --rmi all -v        # Para bajar airflow
sudo docker compose -f docker-compose-kubernete.yaml down --rmi all -v      # Para bajar servicios complementarios
sudo docker compose -f docker-compose-jupyter.yaml down --rmi all -v        # Para bajar jupyterlab
```

###         Revisi√≥n del Estado y Exposici√≥n de los Servicios   

Una vez levantados todos los servicios mediante Docker, iniciamos la etapa de revisi√≥n para asegurarnos de que cada componente se est√° ejecutando correctamente y es accesible desde los puertos configurados en `localhost`.

Podemos validar la exposici√≥n accediendo desde un navegador web a las siguientes direcciones:

```bash 
http://localhost:8989       # fastapi
http://localhost:8501       # streamlit
http://localhost:3000       # grafana
http://localhost:5000       # mlflow
http://localhost:9090       # promehteus
http://localhost:9001       # minio
http://localhost:8888       # jupyter
```

Es importante comprobar que al acceder a cada uno de estos endpoints, los servicios levantan sus respectivas interfaces y que no presentan errores de conexi√≥n, tiempo de espera o conflictos de puerto. Este paso asegura que la arquitectura montada en Docker funciona como se espera antes de avanzar al despliegue en Kubernetes.


Estando dentro del endpoint de minio, creamos un bucket llamado mlflows3, donde se almacenara los artefactos correspondientes a los modelos entrenados a traves de airflow 






```bash 

```


docker compose -f docker-compose-airflow.yaml exec airflow-scheduler ls -l /opt/airflow/dags


```bash 
sudo docker compose -f docker-compose-jupyter.yaml up --build -d
```

```bash
sudo docker compose -f docker-compose-jupyter.yaml down --rmi all -v
```

```bash

```


el primer dag en correr por √∫nica vez es raw_data_initial_batch_load, este DAG realiza la primer carga de los datos en la tabla de la rawdata, con este procedimiento garantizamos que se carga los datos a la rawdata y esperamos a que termine, s√≥lo se necesita correr una primera vez

Luego seguimos con el DAG clean_data_pipeline, el cual se encarga de hacer la carga de datos de la rawdata, hace la limpieza de los mismos y los carga, es importante s√≥lo encenderlo despu√©s de que finalice raw_data_initial_batch_load, as√≠ garantizamos que el flujo de consumo de datos quede activo, adicional, tenemos las columnas load_date y processed_date que son las columnas que nos permiten hacer seguimiento de la carga y flujo de datos en las cargas de datos


docker exec -it proyecto3-mlops-postgres-1 bash
psql -U airflow -d airflow
psql -U admin -d supersecret
\dn
\dt raw_data.*
SELECT COUNT(*) FROM raw_data.diabetes_raw;
DROP TABLE raw_data.diabetes_raw;
SELECT COUNT(*) FROM raw_data.diabetes_clean;
DROP TABLE raw_data.diabetes_clean;



Luego entramos a minio, y ac√° creamos el bucket `mlflows3` para que se guarden los objetos del proceso de entrenamiento de modelos del experimento

docker exec -it mlflow bash
touch test_s3.py
echo "import os, mlflow
os.environ['AWS_ACCESS_KEY_ID'] = 'admin'
os.environ['AWS_SECRET_ACCESS_KEY'] = 'supersecret'
os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http://minio:9000'
mlflow.set_tracking_uri('http://mlflow:5000')
with mlflow.start_run():
    with open('/tmp/hello.txt','w') as f: f.write('hola')
    mlflow.log_artifact('/tmp/hello.txt')
    print('Artifact URI:', mlflow.get_artifact_uri())
    print('‚úÖ Artifact logged')
" > test_s3.py
python test_s3.py





Usando jupyter podemos examinar los datos cargados dentro del esquema, donde verificamos en los descriptivos la informaci√≥n junto con la documentaci√≥n de la misma, as√≠ mismo, podemos darnos cuenta que el conjunto de datos est√° supremamente bien documentado, por lo tanto, consideramos que s√≥lo es necesario hacer el trabajo para corregir los car√°cteres especiales, o valores "faltantes" que nos permita hacer una adecuaci√≥n del conjunto de datos y cargarlos en el clean data 
https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008



Notas

para disminuir el consumo de contenedor al m√≠nimo posible para poderlo desplegar en las m√°quinas, usaremos un solo postgres

levantamos streamlit dentro de un contenedor considerando que este paso es f√°cil de hacer para continuar con la conterinizaci√≥n de los servicios a usar 

