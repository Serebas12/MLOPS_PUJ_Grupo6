#       Desarrollo Proyecto 2

Este repositorio contiene los archivos y configuraciones necesarios para el despliegue de un entorno completo que integra los servicios de **Airflow**, **MLflow**, **MinIO**, **FastAPI**, **MySQL** y **Streamlit**. Este ecosistema permite implementar un flujo de trabajo integral para la ingesta, procesamiento, modelado, registro y despliegue de modelos de machine learning, todo gestionado desde un DAG orquestado con Airflow.

La arquitectura propuesta permite explorar y aplicar principios de MLOps, haciendo Ã©nfasis en la trazabilidad, el versionamiento y la automatizaciÃ³n del ciclo de vida de los modelos. El flujo del proceso incluye la obtenciÃ³n de datos mediante una **API**, su almacenamiento en una base de datos, el preprocesamiento y entrenamiento de modelos, la selecciÃ³n y publicaciÃ³n del modelo Ã³ptimo en producciÃ³n, su registro con control de versiones a travÃ©s de MLflow, el consumo del modelo desde una API de predicciÃ³n, y su disponibilizaciÃ³n al usuario final mediante una interfaz interactiva construida con Streamlit.

---

##      Estructura del Directorio

A continuaciÃ³n se presenta la estructura del directorio del proyecto, la cual refleja cÃ³mo se organizan los archivos para el despliegue de cada uno de los servicios que componen el entorno. AdemÃ¡s, se incluye una simulaciÃ³n de una fuente de datos externa a travÃ©s de un segundo archivo de orquestaciÃ³n (**docker-compose-external.yaml**), que expone su servicio en un puerto del host.

Este servicio externo es consumido por los contenedores definidos en el archivo principal (**docker-compose-p2.yaml**) mediante una red compartida (**network**) declarada explÃ­citamente. Esta configuraciÃ³n permite la lectura del puerto expuesto en el host desde dentro del ecosistema contenedorizado, garantizando la conectividad entre servicios incluso si pertenecen a diferentes composiciones de Docker.

```plaintext
ğŸ“ PROYECTO2
â”œâ”€â”€ ğŸ“ airflow                          # ConfiguraciÃ³n del orquestador de tareas (Airflow)
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile
â”‚   â””â”€â”€ ğŸ“„ requirements.txt
â”œâ”€â”€ ğŸ“ app                              # API FastAPI para servir el modelo entrenado
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile
â”‚   â”œâ”€â”€ ğŸ“„ main.py
â”‚   â””â”€â”€ ğŸ“„ requirements.txt
â”œâ”€â”€ ğŸ“ dags                             # DAG de entrenamiento, registro y publicaciÃ³n del modelo
â”‚   â””â”€â”€ ğŸ“„ modeling_covertype.py
â”œâ”€â”€ ğŸ“ external                         # API alterna que simula la fuente de datos externa
â”‚   â”œâ”€â”€ ğŸ“ data                         # Datos de ejemplo para el servicio simulado
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile
â”‚   â”œâ”€â”€ ğŸ“„ main.py
â”‚   â””â”€â”€ ğŸ“„ requirements.txt
â”œâ”€â”€ ğŸ“ images                           # Imagenes complementarÃ­as del proceso
â”œâ”€â”€ ğŸ“ logs                             # Archivos generados por Airflow durante la ejecuciÃ³n
â”‚   â”œâ”€â”€ ğŸ“ dag_id=DAG_p2
â”‚   â”œâ”€â”€ ğŸ“„ dag_processor_manager
â”‚   â””â”€â”€ ğŸ“„ scheduler
â”œâ”€â”€ ğŸ“ mlflow                           # ConfiguraciÃ³n adicional para el servidor de MLflow
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile
â”‚   â””â”€â”€ ğŸ“ plugins
â”œâ”€â”€ ğŸ“„ .env                             # Variables de entorno para configuraciÃ³n de servicios
â”œâ”€â”€ ğŸ“„ docker-compose-external.yaml     # OrquestaciÃ³n de la API alterna
â”œâ”€â”€ ğŸ“„ docker-compose-p2.yaml           # Despliegue completo del entorno de producciÃ³n
â”œâ”€â”€ ğŸ“„ README.md                        # Este archivo
â””â”€â”€ ğŸ“„ streamlit_app.py                 # Interfaz de usuario para consumo del modelo
```

Esta organizaciÃ³n modular permite una gestiÃ³n eficiente de cada componente del flujo de datos y facilita la escalabilidad del entorno.

---

##      Despliegue de API Alterna

Iniciamos con el despliegue de la API que simula la entrega de datos, la cual se ejecuta en el host y representa la fuente externa de informaciÃ³n que alimenta el flujo de trabajo MLOps implementado en este experimento. Este servicio fue proporcionado por el profesor como base para poder interactuar con una API realista durante el desarrollo del proyecto.

Contar con una fuente de datos activa es fundamental para el funcionamiento del DAG de Airflow, ya que este serÃ¡ responsable de orquestar la ingesta, el procesamiento y el entrenamiento del modelo. La API garantiza una entrega periÃ³dica de datos que simula un entorno de producciÃ³n o streaming.

Para levantar este servicio, utilizamos el siguiente comando desde la terminal:

```bash
sudo docker compose -f docker-compose-external.yaml up --build -d
```

Para verificar que el servicio se encuentra en lÃ­nea y operativo, podemos ejecutar los siguientes comandos desde la terminal. Estos permiten comprobar que el servidor responde correctamente tanto a una solicitud base como a la entrega de datos parametrizada:

```bash
curl http://localhost:80/
curl "http://localhost:80/data?group_number=6"
```

El primer comando valida que el servicio estÃ© activo, mientras que el segundo confirma que el endpoint de entrega de datos estÃ© funcionando y retornando informaciÃ³n en formato JSON, lo cual es esencial para la ingesta dentro del DAG de Airflow.

En contraparte, para dar de baja el servicio, eliminando tambiÃ©n los volÃºmenes asociados y las imÃ¡genes generadas durante el despliegue, ejecutamos el siguiente comando desde la terminal:

```bash
sudo docker compose -f docker-compose-external.yaml down -v --rmi all
```
Este procedimiento garantiza una limpieza completa del entorno relacionado con el servicio de entrega de datos, permitiendo su redeploy sin conflictos residuales en futuras ejecuciones.

---

##      Despliegue del Esquema Completo

Se orquestan los siguientes servicios:
- **Airflow**: planificaciÃ³n y ejecuciÃ³n del pipeline
- **MySQL (x2)**: almacenamiento de datos suministrados y metadata de MLflow
- **MLflow**: tracking, registro y gestiÃ³n del modelo
- **MinIO**: almacenamiento de artefactos con compatibilidad S3
- **FastAPI**: API REST para servir el modelo entrenado

##      Comandos para Levantar el Entorno

Los siguientes comandos son necesarios para el correcto despliegue del entorno. En primer lugar, se inicializa la configuraciÃ³n base de Airflow mediante su proceso de airflow-init, y posteriormente se levantan todos los servicios definidos en el archivo de orquestaciÃ³n, incluyendo la red compartida (network) que interconecta los contenedores, asÃ­ como el acceso al servicio externo de datos expuesto en el host:

```bash
sudo docker compose -f docker-compose-p2.yaml up airflow-init
sudo docker compose -f docker-compose-p2.yaml up --build -d
```

Esta secuencia garantiza que la base de datos, los volÃºmenes persistentes, la red interna y los contenedores se encuentren correctamente configurados y en ejecuciÃ³n, permitiendo asÃ­ el funcionamiento integral del flujo MLOps.

El siguiente comando permite desmontar por completo el entorno, eliminando todos los contenedores, volÃºmenes persistentes e imÃ¡genes generadas durante su despliegue. Es Ãºtil en caso de requerir una reconstrucciÃ³n limpia del entorno desde cero:

```bash
sudo docker compose -f docker-compose-p2.yaml down -v --rmi all
```

Esta operaciÃ³n asegura que no queden residuos de configuraciones anteriores, evitando posibles conflictos al volver a levantar los servicios.


##      Accesos a servicios

A continuaciÃ³n, se listan los servicios definidos dentro del archivo docker-compose-p2.yaml, los cuales conforman el entorno completo del experimento:

| Servicio     | URL                       | Usuario   | ContraseÃ±a     |
|--------------|---------------------------|-----------|----------------|
| Airflow      | http://localhost:8080     | airflow   | airflow        |
| MLFlow       | http://localhost:5000     | -         | -              |
| MinIO        | http://localhost:9001     | admin     | supersecret    |
| MySQL(MLFlow)| -                         | mlflow    | mlflow         |
| MySQL(data)  | -                         | admin     | supersecret    |
| FastAPI      | http://localhost:8989     | -         | -              |


##      VerificaciÃ³n de los Servicios   

Para verificar que todos los servicios se encuentran activos y funcionando correctamente, se puede consultar el estado de los contenedores y los puertos expuestos mediante el siguiente comando:

```bash
docker ps -a
```
Este comando lista todos los contenedores en ejecuciÃ³n (y detenidos), junto con informaciÃ³n relevante como el nombre del contenedor, su estado y los puertos mapeados hacia el host. Esto permite confirmar que los servicios estÃ¡n levantados y accesibles desde las interfaces correspondientes.

En caso de que algÃºn contenedor presente errores o no se encuentre en ejecuciÃ³n (estado "exited" o "dead"), se recomienda revisar sus logs para identificar la causa del fallo. Esto se puede hacer mediante el siguiente comando:

```bash
docker logs <nombre-del-contenedor>
```

Es importante tener en cuenta que el contenedor airflow-init-1 puede aparecer en estado exited sin que ello represente un error, ya que su Ãºnica funciÃ³n es inicializar la configuraciÃ³n del entorno durante el arranque. Una vez completada esta tarea, se detiene automÃ¡ticamente, y no requiere intervenciÃ³n adicional.

Para verificar que MLflow y su conexiÃ³n con la base de datos de metadatos (MySQL) se hayan levantado correctamente, se puede inspeccionar el log del contenedor correspondiente utilizando el siguiente comando:

```bash
docker logs mlflow
```
Dentro de los registros de salida, es fundamental identificar las siguientes lÃ­neas, que confirman una conexiÃ³n exitosa con la base de datos y la correcta inicializaciÃ³n del entorno de tracking:

```text
INFO [alembic.runtime.migration] Context impl MySQLImpl.
INFO [alembic.runtime.migration] Running upgrade  -> <hash>
```

La apariciÃ³n de estos mensajes indica que MLflow ha logrado conectarse al motor MySQL configurado como metadata store y que las migraciones de esquema se han aplicado correctamente. Si estos mensajes no aparecen, o se observan errores previos de conexiÃ³n, se recomienda revisar los parÃ¡metros de acceso definidos en el archivo docker-compose-p2.yaml y en las variables de entorno del contenedor.

> âš ï¸ Puede que los primeros intentos de conexiÃ³n fallen. Lo importante es que finalmente se observe la conexiÃ³n exitosa en los logs.

---

##      CreciÃ³n del Bucket en MinIO 

Para crear el bucket donde se almacenarÃ¡n los artefactos generados por los modelos entrenados a travÃ©s del DAG de Airflow, accedemos a la interfaz web de MinIO desde cualquier navegador utilizando la siguiente URL:

```bash
http://localhost:9001
```

Al ingresar, se presentarÃ¡ la pÃ¡gina de inicio de sesiÃ³n. Se deben utilizar las credenciales previamente definidas:

-   Usuario: admin

-   ContraseÃ±a: supersecret

Una vez autenticados, navegamos a la opciÃ³n "**Create Bucket**", ubicada en la parte superior derecha de la interfaz. En este paso, es fundamental asignar al bucket el nombre mlflows3, ya que este valor ha sido predefinido en la configuraciÃ³n del entorno y es el que espera MLflow para operar correctamente.

Este bucket actuarÃ¡ como repositorio de artefactos, permitiendo que MLflow almacene los modelos entrenados, mÃ©tricas, parÃ¡metros y otros elementos generados durante la ejecuciÃ³n del pipeline definido en el DAG de Airflow.

![Create Bucket](images/create_bucket.png)

Para verificar que el bucket fue creado correctamente, este deberÃ¡ aparecer en la lista principal del panel de MinIO, tal como se muestra a continuaciÃ³n:

![Bucket MLFlows3](images/mlflows3_bucket.png)

##      ConfiguraciÃ³n de Airflow

Accedemos a la interfaz web de Airflow a travÃ©s de la siguiente URL en el navegador:

```bash
http://localhost:8080
```

Una vez en la pÃ¡gina de inicio de sesiÃ³n, utilizamos las credenciales previamente establecidas para autenticarnos. Luego, nos dirigimos a la secciÃ³n "Admin" > "Connections", desde donde creamos una nueva conexiÃ³n que permitirÃ¡ establecer la comunicaciÃ³n con el servicio de MySQL, encargado del almacenamiento de los datos recolectados desde la API externa.

![Crear ConexiÃ³n](images/nueva_conexion.png)

Esta conexiÃ³n es fundamental para que el DAG pueda interactuar con la base de datos y ejecutar operaciones de lectura y escritura de forma programÃ¡tica durante su ejecuciÃ³n. A continuaciÃ³n, se detallan los parÃ¡metros que deben ser configurados al momento de crear la conexiÃ³n a MySQL dentro de la interfaz de Airflow:

- **Connection ID**: `mysql_default`
- **Connection Type**: `MySQL`
- **Host**: `mysql-data-store`
- **Schema**: `datadb`
- **Login**: `admin`
- **Password**: `supersecret`
- **Port**: `3306`

Una vez ingresados estos datos, se guarda la conexiÃ³n para que estÃ© disponible desde los hooks definidos en el cÃ³digo del DAG. Esta conexiÃ³n serÃ¡ utilizada, por ejemplo, por MySqlHook para insertar registros, consultar datos y verificar la existencia de tablas durante la ejecuciÃ³n del pipeline.

![Activar DAG](images/DAG_p2.png)

Una vez creada la conexiÃ³n, podemos regresar a la pÃ¡gina principal de Airflow y habilitar el DAG llamado **DAG_p2**, encargado de ejecutar de forma orquestada todo el flujo de trabajo del experimento. Este DAG implementa las siguientes tareas:

-   Ingesta de datos desde la API externa.

-   Entrenamiento de un modelo utilizando GridSearchCV para bÃºsqueda de hiperparÃ¡metros.

-   EvaluaciÃ³n del modelo y registro automÃ¡tico de mÃ©tricas en MLflow.

-   PublicaciÃ³n del modelo entrenado en el MLflow Model Registry.

-   PromociÃ³n automÃ¡tica del mejor modelo al stage Production.

-   Almacenamiento de los artefactos del modelo (pipeline, parÃ¡metros, mÃ©tricas) en MinIO.

âš ï¸ Advertencia: Es importante asegurarse de que el DAG no se encuentre ejecutÃ¡ndose mÃºltiples veces en paralelo. El flujo ha sido diseÃ±ado para ejecutarse de forma secuencial, por lo que correr dos instancias simultÃ¡neas puede provocar conflictos en el acceso a recursos compartidos como la base de datos o el registro del modelo en MLflow.

---

##      RevisiÃ³n de los Resultados en MLFlow    

Una vez finalizada la ejecuciÃ³n del DAG en Airflow, accedemos a la interfaz de MLflow a travÃ©s del navegador utilizando la URL:

```bash
http://localhost:5000
```

Dentro de la plataforma, navegamos al experimento denominado **covertype_training_experiment**, donde podremos visualizar la ejecuciÃ³n mÃ¡s reciente del pipeline. Al ingresar a la corrida correspondiente, es posible observar las mÃ©tricas del modelo entrenado, tales como accuracy, precision, recall y f1-score.

Adicionalmente, al acceder en detalle a la ejecuciÃ³n, se pueden consultar los parÃ¡metros utilizados durante el ajuste del modelo, los artefactos generados (como el modelo serializado, el pipeline completo, grÃ¡ficos, etc.) y los registros adicionales almacenados automÃ¡ticamente gracias al uso de **mlflow.sklearn.autolog()**.

Esta trazabilidad permite comparar mÃºltiples ejecuciones y mantener un historial completo del desempeÃ±o de cada configuraciÃ³n de entrenamiento.

![Experimento MLFlow](images/mlflow_experiment.png)

Una vez verificado el experimento desarrollado, tambiÃ©n es posible comprobar que el modelo ha sido promovido a producciÃ³n. Para ello, accedemos a la pestaÃ±a **Models** dentro de la interfaz de MLflow, donde encontraremos el modelo registrado bajo el nombre modelo_covertype_vf.

Al hacer clic sobre el nombre del modelo, se despliega la lista de versiones registradas. La versiÃ³n mÃ¡s reciente (generada por el DAG) deberÃ¡ estar marcada con el estado Production, lo que indica que ha sido seleccionada como la versiÃ³n oficial para ser consumida por otros servicios.

Al ingresar a la versiÃ³n especÃ­fica, podemos observar detalles adicionales como el (run_id), el experimento de origen, las mÃ©tricas asociadas, el autor del registro y la ubicaciÃ³n de los artefactos almacenados. Esta informaciÃ³n es clave para mantener trazabilidad y control de versiones dentro del flujo MLOps.

![MLFlow Models](images/mlflow_models.png)

##      RevisiÃ³n de la Disponibilidad del Modelo

Finalmente, accedemos a la API de inferencia desarrollada con FastAPI a travÃ©s del siguiente enlace en el navegador o utilizando herramientas como curl o Postman:

```bash
http://localhost:8989/docs
```

Esta API se encuentra conectada directamente al MLflow Model Registry y ha sido configurada para consumir la versiÃ³n del modelo que se encuentra en estado Production. Desde esta interfaz, es posible enviar datos de entrada al modelo entrenado y recibir una predicciÃ³n como respuesta, validando asÃ­ que el flujo de entrenamiento, publicaciÃ³n y despliegue fue exitoso.

Esta API actÃºa como puente entre el modelo publicado y los servicios o usuarios finales que requieren realizar inferencias sobre nuevos datos.

![FastAPI](images/fastapi.png)

##      VisualizaciÃ³n e Inferencia con Streamlit

Como complemento al entorno desarrollado, se implementÃ³ una interfaz grÃ¡fica de usuario (UI) utilizando Streamlit, con el objetivo de permitir que un usuario final pueda interactuar fÃ¡cilmente con el modelo entrenado y realizar predicciones de forma manual e intuitiva.

Primero, asegÃºrese de que el entorno de desarrollo cuente con la biblioteca Streamlit instalada. Luego, para iniciar la interfaz, ejecute el siguiente comando en la terminal:

```bash
streamlit run streamlit_app.py --server.port 8503
```
Una vez iniciado el servicio, la aplicaciÃ³n estarÃ¡ disponible en el navegador accediendo a la URL:

```bash
http://localhost:8503
```

Desde esta interfaz, el usuario puede ingresar manualmente los valores de entrada requeridos por el modelo y obtener la predicciÃ³n en tiempo real. Por defecto, el comando abre automÃ¡ticamente el navegador al iniciar el servidor y permanece activo mientras se mantenga la terminal abierta. La interfaz de streamlit se verÃ¡ de la siguiente forma: 

<div align="center">
  <img src="images/streamlit2.png" alt="streamlit" width="700"/>
</div>

Streamlit estÃ¡ completamente integrado con las APIs proporcionadas previamente mediante FastAPI. Los detalles de esta integraciÃ³n se pueden consultar en el archivo [`streamlit_app.py`](./streamlit_app.py) presente en este repositorio. En la secciÃ³n **Modelos**, la aplicaciÃ³n invoca automÃ¡ticamente el mÃ©todo `listar_modelos`, el cual muestra el listado completo de los modelos desplegados en MLflow y disponibles para su uso.  Adicionalmente, la interfaz permite ingresar los valores correspondientes a cada variable para realizar la predicciÃ³n; por defecto, las variables numÃ©ricas se inicializan en `0` y las categÃ³ricas se dejan en blanco.

Por Ãºltimo, se confirma que se implementaron validaciones en algunos de los campos para garantizar la integridad de los datos ingresados. En caso de que los valores no cumplan con las especificaciones definidas para cada variable, se mostrarÃ¡ un mensaje de error similar al siguiente:

<div align="center">
  <img src="images/streamlit3.png" alt="streamlit" width="700"/>
</div>

Se implementaron las siguientes validaciones:

1. **Valores no negativos:** Ninguna variable puede tener un valor numÃ©rico negativo.
2. **Variables categÃ³ricas obligatorias:** Los campos categÃ³ricos no pueden estar vacÃ­os.
3. **Rangos vÃ¡lidos:** SegÃºn la naturaleza de cada variable y la documentaciÃ³n del conjunto de datos, se definieron rangos vÃ¡lidos que limitan los valores aceptados.

Si alguno de los datos ingresados no cumple con estas condiciones, se mostrarÃ¡ un mensaje de error claro y descriptivo que guÃ­a al usuario en la correcciÃ³n del dato.

AdemÃ¡s, en caso de que el API llegue a fallar por cualquier motivo, la interfaz de Streamlit estÃ¡ configurada para capturar y mostrar el error correspondiente, permitiendo que el desarrollador pueda identificar rÃ¡pidamente la causa del problema y tomar acciones correctivas.

Aunque en este experimento la aplicaciÃ³n fue ejecutada localmente, el servicio puede ser fÃ¡cilmente **dockerizado** e integrado en futuros entornos contenedorizados, permitiendo su despliegue como parte de flujos de trabajo completos en ambientes de producciÃ³n.

Adicionalmente, es posible integrarlo con herramientas como **Ngrok**, lo cual facilita el acceso a la aplicaciÃ³n desde diferentes equipos mediante la publicaciÃ³n de una **URL pÃºblica**, Ãºtil especialmente durante fases de desarrollo colaborativo, pruebas o demostraciones remotas.

---

##      Aclaraciones del CÃ³digo

Este proyecto nace a partir de las bases proporcionadas por el profesor. Se realizaron ajustes clave para permitir:

- Un entorno dockerizado y replicable
- Un esquema de orquestaciÃ³n con dependencias entre servicios
- Entrenamiento supervisado automatizado vÃ­a Airflow
- Registro de mÃ©tricas y artefactos en MLflow y MinIO
- PromociÃ³n automÃ¡tica del mejor modelo al stage "Production"
- Consumo del modelo vÃ­a FastAPI y visualizaciÃ³n con Streamlit

Gracias a este enfoque, se garantiza la reproducibilidad del experimento, la trazabilidad de los modelos y una estructura modular para posibles integraciones futuras en ambientes de desarrollo reales o en producciÃ³n.

