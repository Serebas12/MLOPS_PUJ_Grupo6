#       Desarrollo Taller Airflow

Este repositorio contiene los archivos y configuraciones necesarias para el despliegue de un entorno de **Apache Airflow**, dise√±ado para gestionar la ejecuci√≥n de flujos de trabajo automatizados en el procesamiento de datos y entrenamiento de modelos de Machine Learning.

El **pipeline** implementado est√° estructurado mediante tasks definidas dentro de un **DAG** (**Directed Acyclic Graph**), asegurando una ejecuci√≥n controlada y reproducible de cada etapa del proceso. El flujo inicia con la limpieza de la base de datos. Luego, se realiza la carga de informaci√≥n desde el archivo fuente hacia la base de datos. Posteriormente, los datos son preprocesados mediante la transformaci√≥n y normalizaci√≥n de las variables, aplicando codificaci√≥n a los atributos categ√≥ricos y escalando las variables num√©ricas mediante la normalizaci√≥n est√°ndar. Una vez completada esta etapa, se lleva a cabo el entrenamiento del modelo utilizando un algoritmo de aprendizaje supervisado (Random Forest). Finalmente, se realiza la validaci√≥n del modelo, evaluando su desempe√±o con el **accuracy** (precisi√≥n).

Las tareas descritas anteriormente est√°n embebidas en tasks dentro del DAG de Airflow, permitiendo un control detallado sobre su ejecuci√≥n y facilitando la trazabilidad del proceso.

##      Estructura del Directorio:

```plaintext
üìÅ taller_airflow 
|‚îÄ‚îÄ üìÅapp                                       #   Carpeta donde se almacena los recursos para desplegar la API de inferencia
    |‚îÄ‚îÄ üìÑdockerfile                            #   Dockerfile para hacer la carga del ambiente para la API con FastAPI
    |‚îÄ‚îÄ üìÑmain.py                               #   Archivo Python donde se almacena el c√≥digo que se implementa la API
    |‚îÄ‚îÄ üìÑrequirements.txt                      #   Dependencias para el despliegue de la API
|‚îÄ‚îÄ üìÅdags                                      #   Carpeta donde se almacenan los DAGS, adem√°s de almacenar la rawdata
    |‚îÄ‚îÄ üìÑpenguins_size.csv                     #   Archivo para el experimento penguins_size.csv (rawdata)
    |‚îÄ‚îÄ üìÑproceso_modelo.py                     #   DAG del experimento
|‚îÄ‚îÄ üìÅimages                                    #   Imagenes de soporte para el README.md
    |‚îÄ‚îÄ üìÑCodeDAG.png                     
    |‚îÄ‚îÄ üìÑDAG.png                     
    |‚îÄ‚îÄ üìÑNewConnection.png                     
    |‚îÄ‚îÄ üìÑTasksDAG.png                     
    |‚îÄ‚îÄ üìÑTriggerDAG.png                     
|‚îÄ‚îÄ üìÅlogs                                      #   Almacen de los logs del experimento
|‚îÄ‚îÄ üìÅplugins                                   
|‚îÄ‚îÄ üìÑ.env                                      #   Variable de entorno para lanzar Apache Airflow
|‚îÄ‚îÄ üìÑdocker-compose.yaml                       #   Docker Compose que almacena el c√≥digo para el despliegue del experimento
|‚îÄ‚îÄ üìÑdockerfile                                #   Dockerfile para cargar las dependencias necesarias (no se usar docker compose)
|‚îÄ‚îÄ üìÑREADME.md                                 
|‚îÄ‚îÄ üìÑrequirements.txt                          #   Dependencias para el DAG
```

**Nota:** Otras carpetas y objetos son producto de la ejecuci√≥n del experimento, por ende no se incluyen dentro de la estructura del directorio.

##      Requisitos Previos  

Antes de iniciar con el experimento, es necesario contar con una m√°quina con las siguientes condiciones:

-       Docker y Docker-Compose (para garantizar entornos reproducibles y aislados).

-       Un entorno de desarrollo o terminal compatible con Docker, como:

    -       VS Code (recomendado con la extensi√≥n Remote - Containers).

    -       PyCharm (con soporte para Docker).

    -       Cualquier terminal que permita la ejecuci√≥n de Docker y la gesti√≥n de contenedores.

**Nota:** No se requieren requisitos adicionales, ya que el despliegue de Airflow mediante Docker Compose incluye una imagen preconfigurada con todos los recursos necesarios para la ejecuci√≥n del DAG. Esto garantiza que Python y sus respectivas dependencias est√©n correctamente instaladas y disponibles en el entorno de ejecuci√≥n, evitando conflictos de versiones y facilitando la portabilidad del sistema.

##      Proceso de Ejecuci√≥n 

Para iniciar el entorno de Airflow, es fundamental mantener la estructura de directorios establecida durante el desarrollo del experimento. Esto incluye la correcta configuraci√≥n de la variable de entorno almacenada en .env y la presencia de los directorios dags, logs y plugins, los cuales son esenciales para el funcionamiento adecuado del sistema.

Si estos directorios no existen, pueden ser creados manualmente o mediante el siguiente comando en VS Code, que fue el entorno utilizado para este desarrollo:

```Bash
mkdir dags, logs, plugins
echo "AIRFLOW_UID=50000" | Out-File -Encoding utf8 .env
```
**Nota:** Este comando asegura la creaci√≥n de las carpetas requeridas y el archivo de configuraci√≥n del entorno, permitiendo que Airflow se ejecute sin inconvenientes. Pero no recomendamos su ejecuci√≥n si se clona este repositorio, pues se podr√≠a modificar el contenido que contiene la carpeta dags, y se perder√≠a tanto la rawdata como el DAG del experimento.

Tomando en cuenta que los directorios dags, logs, y plugins ya existen, y que la variable de entorno requerida para iniciar Airflow ha sido correctamente definida, el procedimiento para desplegar Airflow se realiza ejecutando los siguientes comandos en la consola de la herramienta utilizada para reproducir este experimento:

```Bash
docker compose up airflow-init
docker compose up --build -d
```
El primer comando **docker compose up airflow-init** prepara el entorno de Airflow y sus dependencias antes de su ejecuci√≥n. Una vez completada esta configuraci√≥n, el segundo comando **docker compose up --build -d** ejecuta Airflow en segundo plano, permitiendo que los servicios corran de manera persistente sin bloquear la terminal y asegurando que la imagen creada este actualizada con las especificaciones proporcionadas en docker-compose.yaml.

Este procedimiento garantiza que Airflow se levante correctamente y est√© listo para ejecutar el DAG definido para este experimento.

##      Dentro del DAG

Una vez iniciado Airflow, se accede a la interfaz de usuario a trav√©s del navegador ingresando la siguiente direcci√≥n:

```Bash
http://localhost:8080
```

Al ingresar, el sistema solicitar√° credenciales de acceso, las cuales est√°n predefinidas de la siguiente manera:

-   Usuario: airflow

-   Contrase√±a: airflow

Esta interfaz permite visualizar, gestionar y monitorear los DAGs definidos en Airflow, as√≠ como revisar los registros de ejecuci√≥n de cada tarea, modificar configuraciones y ejecutar manualmente los flujos de trabajo seg√∫n sea necesario.

Dentro de la interfaz, se encontrar√° el **DAG** denominado **modelo_penguin**, el cual ha sido dise√±ado para ejecutar el pipeline descrito previamente. Este DAG orquesta las distintas fases del proceso, asegurando la correcta ejecuci√≥n de la limpieza de la base de datos, carga de informaci√≥n, preprocesamiento, entrenamiento y validaci√≥n del modelo.

![DAG Creado](images/DAG.png)

Antes de ejecutar el DAG, es necesario configurar una nueva conexi√≥n en Airflow, la cual debe apuntar al motor de base de datos **MySQL**, que se encuentra ejecut√°ndose como un servicio dentro de **Docker Compose**.

Para realizar esta configuraci√≥n, dentro de la interfaz de Airflow, se debe seguir el siguiente procedimiento:

-   Ir al men√∫ Admin > Connections.

-   Hacer clic en el √≠cono + para agregar una nueva conexi√≥n.

-   Completar los campos con los par√°metros de la base de datos MySQL definida en el archivo docker-compose.yml.

Esta configuraci√≥n es crucial, ya que Airflow necesita esta conexi√≥n para interactuar con la base de datos y ejecutar correctamente las tareas de carga, preprocesamiento y almacenamiento de resultados.

![Nueva Conexi√≥n](images/NewConnection.png)

La lista de par√°metros para generar la nueva conexi√≥n son: 

-   Connection Id: mysql_default

-   Connection Type: MySQL

-   Description: Conexi√≥n para cargar los conjuntos de datos externos ‚Äúpenguins_size.csv‚Äù

-   Host: mysql

-   Schema: mydatabase

-   Login:  mysqluser

-   Password: airflow

-   Port: 3306

Una vez configurados los par√°metros de conexi√≥n, es necesario realizar una prueba de conectividad para asegurarse de que Airflow puede establecer comunicaci√≥n con la base de datos MySQL.

Para ello, dentro del formulario de configuraci√≥n de la conexi√≥n en Airflow, se debe hacer clic en el bot√≥n "Test". Si la conexi√≥n es exitosa, se mostrar√° un mensaje de confirmaci√≥n.

Despu√©s de haber aprobado la prueba, procedemos a guardar la conexi√≥n, asegurando que estar√° disponible para su uso dentro del DAG modelo_penguin y otros flujos de trabajo que requieran acceso a la base de datos.

Para acceder al c√≥digo fuente del DAG compilado, debemos regresar a la p√°gina principal de Airflow, en la secci√≥n DAGs.

-   Ubicar el DAG modelo_penguin dentro de la lista de DAGs.

-   Hacer clic sobre el nombre del DAG para ingresar a su vista detallada.

-   Navegar a la pesta√±a <> Code, donde se mostrar√° el c√≥digo fuente del DAG cargado en el entorno de Airflow.

Desde esta vista, es posible inspeccionar la implementaci√≥n del DAG, revisar la estructura de sus tasks y validar su configuraci√≥n antes de su ejecuci√≥n.

![Code DAG](images/CodeDAG.png)

En el c√≥digo del DAG, se pueden observar cuatro funciones principales que gestionan todo el proceso del pipeline, asegurando la ejecuci√≥n estructurada de cada etapa.

1.   Borrado del esquema: Se encarga de eliminar por completo el esquema utilizado en la base de datos, garantizando que el entorno est√© limpio antes de cada ejecuci√≥n.

2.  Creaci√≥n de la tabla para la rawdata: Genera la estructura necesaria en la base de datos para almacenar la informaci√≥n externa en su estado original.

3.  Carga de la rawdata: Inserta los datos en la tabla creada, permitiendo su posterior transformaci√≥n y an√°lisis.

4.  Preprocesamiento, entrenamiento y validaci√≥n del modelo: Aplica las transformaciones necesarias a los datos, entrena un modelo de aprendizaje supervisado y realiza su evaluaci√≥n. En esta etapa, el modelo entrenado es serializado y almacenado en un archivo **.pkl**, lo que permite su reutilizaci√≥n y facilita su integraci√≥n con una **API** en **FastAPI** para realizar predicciones en tiempo real.

Este flujo de trabajo permite la ejecuci√≥n automatizada del pipeline y garantiza la trazabilidad de cada fase, asegurando la reproducibilidad de los resultados.

![Definidi√≥n del Orden de Ejecuci√≥n de los Tasks](images/TasksDAG.png)

Para ejecutar completamente el DAG, podemos regresar a la p√°gina principal de Airflow, en la pesta√±a DAGs, y localizar el DAG modelo_penguin dentro de la lista de DAGs disponibles.

Una vez identificado, hacemos clic en la acci√≥n **"Trigger DAG"**, lo que iniciar√° la ejecuci√≥n secuencial de todas las tasks del pipeline. Esto permitir√° que el proceso de borrado del esquema, creaci√≥n de la tabla, carga de datos, preprocesamiento, entrenamiento y validaci√≥n del modelo se lleve a cabo de manera automatizada.

Al finalizar la ejecuci√≥n, el modelo estar√° disponible y almacenado como un archivo .pkl, listo para ser utilizado mediante la API desarrollada en FastAPI.

![Trigger DAG](images/TriggerDAG.png)

##      Uso de la API

Para hacer uso de la API de FastAPI, accedemos a trav√©s del navegador ingresando la siguiente direcci√≥n:

```Bash
http://localhost:8989
```

Esto nos mostrar√° la interfaz generada autom√°ticamente por FastAPI, donde se encuentran documentados los m√©todos GET y POST disponibles.

-   **M√©todo GET**: Permite rastrear y verificar el modelo entrenado por el DAG, asegurando que est√© correctamente almacenado y disponible para su uso.

-   **M√©todo POST**: Facilita el env√≠o de datos al modelo para realizar predicciones, utilizando el modelo previamente entrenado y serializado en el pipeline de Airflow.

En esta fase, retomamos el experimento previo en el que se desarroll√≥ una API para el despliegue y uso de modelos entrenados con la librer√≠a Scikit-Learn, asegurando que el flujo de trabajo implementado en Airflow pueda integrarse de manera eficiente con este servicio.

##  Aclaraciones del C√≥digo

Para el desarrollo de este experimento, nos apoyamos en los recursos proporcionados por el profesor, adaptando la configuraci√≥n de Docker Compose para incluir un servicio de MySQL, sin modificar ni eliminar el servicio de PostgreSQL, que mantiene su conexi√≥n con Airflow para su correcta inicializaci√≥n.

Adem√°s, se integr√≥ un servicio de FastAPI, permitiendo la utilizaci√≥n del modelo entrenado en el DAG para realizar predicciones en tiempo real. Tambi√©n se agreg√≥ un volumen persistente para garantizar el correcto uso de MySQL como base de datos externa, asegurando la persistencia de los datos durante el desarrollo del experimento.

A lo largo del c√≥digo, se han incluido comentarios que detallan cada paso en la construcci√≥n del experimento, facilitando su comprensi√≥n y reproducci√≥n.